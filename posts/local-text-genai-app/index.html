<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta
    name="description"
    content=""
  />
  
    
      <title>Building a Local GenAI Application using TinyLlama | Prabhsurat&#39;s Blog</title>
    
  
  <link rel="stylesheet" href="/css/reset.css"/>
  <link rel="stylesheet" href="/css/font.css"/>
  <link rel="stylesheet" href="/css/smigle.css"/>
  
    <link rel="stylesheet" href="/css/custom.css"/>
  
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" href="/favicon.ico" type="image/x-icon">
  <link rel="manifest" href="/site.webmanifest">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="theme-color" content="#ffffff">
</head>

  <body>
    <div id="root">
      <header>
  <div id="brand">
    <a class="icon-link" href="https://prabhsuratsingh.github.io/">
      <img
        class="icon"
        src="/images/brand-icon.svg"
      />
    </a>
    <div class="text">
      <a href="https://prabhsuratsingh.github.io/"><h1>Prabhsurat&#39;s Blog</h1></a>
      <h3>Software Engineer | Android Native Developer</h3>
    </div>
  </div>
  <nav>
    
      
        
        <a href="/"><b>Home</b></a>
      
         | 
        <a href="/about/"><b>About</b></a>
      
         | 
        <a href="/projects/"><b>Projects</b></a>
      
         | 
        <a href="/posts/"><b>Posts</b></a>
      
         | 
        <a href="/index.xml"><b>RSS</b></a>
      
    
  </nav>
  <hr />
</header>

      <div id="content">
        
  <main>
    <article>
      <h1 class="title">Building a Local GenAI Application using TinyLlama</h1>
      
      <div class="post-meta">
  <strong>
    <span>Posted on</span>
    <time>2025-11-28</time>
    
    
  </strong>
  <span> • 1012 words</span>
  <span> • 5 minute read</span>
  
  
</div>

      <div class="content"><p>The other day I thought of building a very simple and minimal GenAI chat application, using Python only. Well naturally, <strong>FastAPI</strong> was my premier choice for
creating the backend and serving the model. As for keeping it minimal, what better to choose than <strong>Streamlit</strong>.</p>
<p>Now that the choice for serving the model and client was decided, the only thing that remained was,</p>
<blockquote>
<p><strong>&ldquo;Which model do I use? Do I go for some API?&rdquo;</strong></p>
</blockquote>
<p>But that came across a very, very common paradigm of building such chatbots. So lets tweak it a bit!</p>
<p>I decided to use a local, and a very lightweight model, for the generative task. I mean, I got a 1650 with a 10th gen I5, no way im running Deepseek on this one!</p>
<p>These struggles (pain!) made <strong>TinyLlama&rsquo;s 1.1B</strong> chat model, the perfect choice for this application.</p>
<p>With everything in place,</p>
<ul>
<li><strong>TinyLlama</strong> for text generation,</li>
<li><strong>FastAPI</strong> for serving the model,</li>
<li><strong>Streamlit</strong> for client.</li>
</ul>
<p>we are ready to begin coding this application.
I will be using <strong>uv</strong> as my package manager for this application. If you do not have <strong>uv</strong> installed, you can hop onto this website</p>
<p><a href="https://docs.astral.sh/uv/getting-started/installation/">https://docs.astral.sh/uv/getting-started/installation/</a></p>
<p>and install it as per your operating system.</p>
<p>Once that&rsquo;s done, you can verify the installation by running</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>uv --version
</span></span></code></pre></div><p>If you get an output similar to this</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>uv 0.9.8
</span></span></code></pre></div><p>congratulations, your uv installation was successful.</p>
<p>Lets create the directory where we will place all of our code and init a uv repository</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>mkdir tinyllama-chat-app
</span></span><span style="display:flex;"><span>cd tinyllama-chat-app
</span></span><span style="display:flex;"><span>uv init
</span></span></code></pre></div><p>Once this is done, you will quite a bit files created by uv. Nothing to be overwhelmed by, these are just files uv uses to setup your environment. For example
<strong>pyproject.toml</strong> is similar to <strong>requirements.txt</strong>, because it containes all the libraries along with the version needed to setup this project, along with other metadata.</p>
<p>Now lets create the environment we will work in by running uv sync</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>uv sync
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># To activate venv</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 1. Windows CMD </span>
</span></span><span style="display:flex;"><span>venv/Scripts/activate
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 2. Windows Powershell</span>
</span></span><span style="display:flex;"><span>venv<span style="color:#ae81ff">\S</span>cripts<span style="color:#ae81ff">\A</span>ctivate.ps1
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 3. Linus/WSL/macOS</span>
</span></span><span style="display:flex;"><span>source venv/bin/activate
</span></span></code></pre></div><p>This will create a python environment by the name <strong>venv</strong> and activate it. This is where all the libraries you install for this project will live. Why is this preferred? Mainly because this isolates your project from global dependencies, which may cause verison contflicts etc etc. In a nutshell,</p>
<blockquote>
<p><strong>env -&gt; good</strong></p>
</blockquote>
<blockquote>
<p><strong>global libs -&gt; bad</strong></p>
</blockquote>
<p>Now its time to install all the necessary libraries for this application</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>uv add transformers torch streamlit fastapi uvicorn<span style="color:#f92672">[</span>standard<span style="color:#f92672">]</span> requests
</span></span></code></pre></div><p>Now all that&rsquo;s left to setup is the project structure, so lets get right into it</p>
<pre tabindex="0"><code>tinyllama-chat-app/
├── venv/
├── .gitignore
├── .python-version
├── main.py              # this is where the FastAPI service will live
├── model.py             # this is where the TinyLlama model will live
├── client.py            # this is where the Streamlit application will live
├── pyproject.toml
├── README.md
├── uv.lock
</code></pre><p>Now comes the most interesting part of this project, the code (laughs in evil!).</p>
<p>We&rsquo;ll start off by adding <strong>TinyLlama</strong>, or more precisely <strong>TinyLlama/TinyLlama-1.1B-Chat-v1.0</strong>. You can access the model card from HuggingFace from this link,</p>
<p><a href="https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0">https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0</a></p>
<p>We will integrate it through HF&rsquo;s transformers library that we imported while setting up the project.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># model.py</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> transformers <span style="color:#f92672">import</span> Pipeline, pipeline
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Checks system for CUDA, if available, uses GPU else defaults to CPU</span>
</span></span><span style="display:flex;"><span>device <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>device(<span style="color:#e6db74">&#34;cuda&#34;</span> <span style="color:#66d9ef">if</span> torch<span style="color:#f92672">.</span>cuda<span style="color:#f92672">.</span>is_available() <span style="color:#66d9ef">else</span> <span style="color:#e6db74">&#34;cpu&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># We load the model using pipeline, and set the model name, as given in HF.</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">load_tinyllama</span>():
</span></span><span style="display:flex;"><span>    pipe <span style="color:#f92672">=</span> pipeline(
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;text-generation&#34;</span>,  <span style="color:#75715e"># we set the task as &#34;text-generation&#34;</span>
</span></span><span style="display:flex;"><span>        model<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;TinyLlama/TinyLlama-1.1B-Chat-v1.0&#34;</span>,
</span></span><span style="display:flex;"><span>        torch_dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>float16,  <span style="color:#75715e"># Here we set the type, for precision</span>
</span></span><span style="display:flex;"><span>        device<span style="color:#f92672">=</span>device  
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> pipe
</span></span></code></pre></div><p>We have successfully loaded the model, now its time to make use of it to generate text.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># model.py</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">generate_text</span>(
</span></span><span style="display:flex;"><span>        pipe: Pipeline,
</span></span><span style="display:flex;"><span>        prompt: str,
</span></span><span style="display:flex;"><span>        temperature: float <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.7</span>
</span></span><span style="display:flex;"><span>) <span style="color:#f92672">-&gt;</span> str :
</span></span><span style="display:flex;"><span>    messages <span style="color:#f92672">=</span> [
</span></span><span style="display:flex;"><span>        {
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;role&#34;</span>: <span style="color:#e6db74">&#34;system&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;content&#34;</span>: system_prompt
</span></span><span style="display:flex;"><span>        },
</span></span><span style="display:flex;"><span>        {
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;role&#34;</span>: <span style="color:#e6db74">&#34;user&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;content&#34;</span>: prompt
</span></span><span style="display:flex;"><span>        },
</span></span><span style="display:flex;"><span>    ]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    prompt <span style="color:#f92672">=</span> pipe<span style="color:#f92672">.</span>tokenizer<span style="color:#f92672">.</span>apply_chat_template(
</span></span><span style="display:flex;"><span>        messages,
</span></span><span style="display:flex;"><span>        tokenize<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>,
</span></span><span style="display:flex;"><span>        add_generation_prompt<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    predictions <span style="color:#f92672">=</span> pipe(
</span></span><span style="display:flex;"><span>        prompt,
</span></span><span style="display:flex;"><span>        temperature<span style="color:#f92672">=</span>temperature,
</span></span><span style="display:flex;"><span>        max_new_tokens<span style="color:#f92672">=</span><span style="color:#ae81ff">256</span>,
</span></span><span style="display:flex;"><span>        do_sample<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>        top_k<span style="color:#f92672">=</span><span style="color:#ae81ff">50</span>,
</span></span><span style="display:flex;"><span>        top_p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.95</span>
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    output <span style="color:#f92672">=</span> predictions[<span style="color:#ae81ff">0</span>][<span style="color:#e6db74">&#34;generated_text&#34;</span>]<span style="color:#f92672">.</span>split(<span style="color:#e6db74">&#34;&lt;/s&gt;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&lt;|assistant|&gt;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>)[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> output
</span></span></code></pre></div><p>This completes our TinyLlama setup and text generation utility function. If you are overwhelmed by the parameters we used to get the prediction, don&rsquo;t worry, i was too. But these are just the params we adjust to get the quality, length, type of response we want from the model. More on these in a later blog!</p>
<p>Now it&rsquo;s time to setup the FastAPI server, so let&rsquo;s dive right into it</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># main.py</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> fastapi <span style="color:#f92672">import</span> FastAPI
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> model <span style="color:#f92672">import</span> load_tinyllama, generate_text
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>app <span style="color:#f92672">=</span> FastAPI()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">@app.get</span>(<span style="color:#e6db74">&#34;/generate/text&#34;</span>)
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">serve_tinyllama_controller</span>(prompt: str) <span style="color:#f92672">-&gt;</span> str:
</span></span><span style="display:flex;"><span>    pipe <span style="color:#f92672">=</span> load_tinyllama()
</span></span><span style="display:flex;"><span>    output <span style="color:#f92672">=</span> generate_text(pipe, prompt)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> output
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> __name__ <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;__main__&#34;</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">import</span> uvicorn
</span></span><span style="display:flex;"><span>    uvicorn<span style="color:#f92672">.</span>run(app, host<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;localhost&#34;</span>, port<span style="color:#f92672">=</span><span style="color:#ae81ff">8000</span>)
</span></span></code></pre></div><p>Aaand it&rsquo;s as simple as that, this creates a very simple server that serves our TinyLlama model over the endpoint</p>
<pre tabindex="0"><code>http://localhost:8000/generate/text
</code></pre><p>This leaves just one task, the Streamlit client. As you will see in the next code block, setting up a minimalistic UI is as easy as setting up the model and server.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># client.py</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> requests
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> streamlit <span style="color:#66d9ef">as</span> st  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>st<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#34;TinyLlama Chatbot&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> <span style="color:#e6db74">&#34;messages&#34;</span> <span style="color:#f92672">not</span> <span style="color:#f92672">in</span> st<span style="color:#f92672">.</span>session_state:
</span></span><span style="display:flex;"><span>    st<span style="color:#f92672">.</span>session_state<span style="color:#f92672">.</span>messages <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> message <span style="color:#f92672">in</span> st<span style="color:#f92672">.</span>session_state<span style="color:#f92672">.</span>messages:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">with</span> st<span style="color:#f92672">.</span>chat_message(message[<span style="color:#e6db74">&#34;role&#34;</span>]):
</span></span><span style="display:flex;"><span>        st<span style="color:#f92672">.</span>markdown(message[<span style="color:#e6db74">&#34;content&#34;</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> prompt <span style="color:#f92672">:=</span> st<span style="color:#f92672">.</span>chat_input(<span style="color:#e6db74">&#34;What can I help you with?&#34;</span>):
</span></span><span style="display:flex;"><span>    st<span style="color:#f92672">.</span>session_state<span style="color:#f92672">.</span>messages<span style="color:#f92672">.</span>append({
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;role&#34;</span>: <span style="color:#e6db74">&#34;user&#34;</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;content&#34;</span>: prompt
</span></span><span style="display:flex;"><span>    })
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">with</span> st<span style="color:#f92672">.</span>chat_message(<span style="color:#e6db74">&#34;user&#34;</span>):
</span></span><span style="display:flex;"><span>        st<span style="color:#f92672">.</span>text(prompt)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>    response <span style="color:#f92672">=</span> requests<span style="color:#f92672">.</span>get(
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;http://localhost:8000/generate/text&#34;</span>,
</span></span><span style="display:flex;"><span>        params<span style="color:#f92672">=</span>{<span style="color:#e6db74">&#34;prompt&#34;</span>: prompt}
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>    response<span style="color:#f92672">.</span>raise_for_status()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">with</span> st<span style="color:#f92672">.</span>chat_message(<span style="color:#e6db74">&#34;assistant&#34;</span>):
</span></span><span style="display:flex;"><span>        st<span style="color:#f92672">.</span>markdown(response<span style="color:#f92672">.</span>text)
</span></span></code></pre></div><p>And this wraps up our entire application. Now all that&rsquo;s left is to run it and test it out. To do that, you&rsquo;ll need to open two terminal windows, or split terminal if you are using VSCode.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># Firing up the FastAPI server</span>
</span></span><span style="display:flex;"><span>python main.py
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># this will serve our model over localhost:8000</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># Starting up the streamlit client</span>
</span></span><span style="display:flex;"><span>streamlit client.py
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># this will automatically open up the client in a new tab in your browser</span>
</span></span></code></pre></div><p>And there you have it, you just successfully build an entirely local ChatBot using TinyLlama, FastAPI and Streamlit. But you are not limited to just text generation, there are plenty of tiny models for audio, image, video generation for you to use, so feel free to check them out on HuggingFace.</p>
</div>
    </article>
  </main>

      </div>
      <footer>
  <hr />
  
    <p id="social">
      Find me around the web:
      <br />
      
        
        <a href="https://github.com/prabhsuratsingh">GitHub</a>
      
         | 
        <a href="https://www.linkedin.com/in/prabhsurat-singh-1868052ab">Linkedin</a>
      
         | 
        <a href="https://x.com/NeonEdge05">Twitter</a>
      
    </p>
  
  <p class="copyright">
    Copyright © 2025
    <a href="https://prabhsuratsingh.github.io/"><strong>Prabhsurat Singh</strong></a>.
    This work is licensed under the
    <a href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a> license.
  </p>
  <p class="builtWith">
    Built with
    <a href="http://www.gohugo.io/">Hugo</a>,
    using the theme
    <a href="https://gitlab.com/ian-s-mcb/smigle-hugo-theme">smigle</a>,
    which was influenced by the theme
    <a href="https://github.com/sumnerevans/smol">smol</a>.
  </p>
</footer>

    </div>
  </body>
</html>
